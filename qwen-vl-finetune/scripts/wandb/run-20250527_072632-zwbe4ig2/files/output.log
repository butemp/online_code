                                                                                                                                                       
[2025-05-27 07:26:48,563] [WARNING] [stage3.py:2148:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 5.8389, 'grad_norm': 557.3707144923457, 'learning_rate': 4.827586206896552e-08, 'epoch': 0.03}
{'loss': 5.9173, 'grad_norm': 497.8157135490814, 'learning_rate': 1.0344827586206897e-07, 'epoch': 0.05}
{'loss': 5.6796, 'grad_norm': 508.51552571580334, 'learning_rate': 1.5862068965517243e-07, 'epoch': 0.07}
{'loss': 5.0166, 'grad_norm': 286.3124229327166, 'learning_rate': 1.9999772265550936e-07, 'epoch': 0.1}
{'loss': 4.3982, 'grad_norm': 160.63720571082698, 'learning_rate': 1.99943071573867e-07, 'epoch': 0.12}
{'loss': 3.9659, 'grad_norm': 111.8471530889197, 'learning_rate': 1.9981559110172944e-07, 'epoch': 0.15}
{'loss': 3.8266, 'grad_norm': 96.82888950453504, 'learning_rate': 1.99615374135232e-07, 'epoch': 0.17}
{'loss': 3.5532, 'grad_norm': 92.4773652246894, 'learning_rate': 1.9934256657422944e-07, 'epoch': 0.2}
{'loss': 3.3596, 'grad_norm': 65.61208744754865, 'learning_rate': 1.9899736721597784e-07, 'epoch': 0.23}
{'loss': 3.3146, 'grad_norm': 64.36238400014884, 'learning_rate': 1.9858002761026912e-07, 'epoch': 0.25}
{'loss': 3.1697, 'grad_norm': 61.05121774850006, 'learning_rate': 1.9809085187612465e-07, 'epoch': 0.28}
{'loss': 3.0919, 'grad_norm': 64.99269645262655, 'learning_rate': 1.9753019648018085e-07, 'epoch': 0.3}
{'loss': 2.9839, 'grad_norm': 64.42144791427192, 'learning_rate': 1.96898469976929e-07, 'epoch': 0.33}
{'loss': 2.881, 'grad_norm': 56.62441269954315, 'learning_rate': 1.96196132710998e-07, 'epoch': 0.35}
{'loss': 2.7812, 'grad_norm': 52.011840409268565, 'learning_rate': 1.9542369648169715e-07, 'epoch': 0.38}
{'loss': 2.6955, 'grad_norm': 45.12031911971794, 'learning_rate': 1.9458172417006346e-07, 'epoch': 0.4}
{'loss': 2.6237, 'grad_norm': 46.01539908272566, 'learning_rate': 1.9367082932868553e-07, 'epoch': 0.42}
{'loss': 2.5686, 'grad_norm': 49.86915651323768, 'learning_rate': 1.9269167573460216e-07, 'epoch': 0.45}
{'loss': 2.4836, 'grad_norm': 49.03056042479545, 'learning_rate': 1.9164497690560243e-07, 'epoch': 0.47}
{'loss': 2.4239, 'grad_norm': 48.78652526128909, 'learning_rate': 1.9053149558027887e-07, 'epoch': 0.5}
{'loss': 2.3487, 'grad_norm': 45.33110678586766, 'learning_rate': 1.8935204316221328e-07, 'epoch': 0.53}
{'loss': 2.293, 'grad_norm': 43.5577531066837, 'learning_rate': 1.8810747912869977e-07, 'epoch': 0.55}
{'loss': 2.2506, 'grad_norm': 47.65810914123112, 'learning_rate': 1.867987104044363e-07, 'epoch': 0.57}
{'loss': 2.2149, 'grad_norm': 45.43913235501668, 'learning_rate': 1.854266907006405e-07, 'epoch': 0.6}
{'loss': 2.2036, 'grad_norm': 49.87504520589999, 'learning_rate': 1.8399241982007208e-07, 'epoch': 0.62}
{'loss': 2.1678, 'grad_norm': 47.60792359057362, 'learning_rate': 1.8249694292846783e-07, 'epoch': 0.65}
{'loss': 2.1336, 'grad_norm': 51.741131017313656, 'learning_rate': 1.8094134979292012e-07, 'epoch': 0.68}
{'loss': 2.1116, 'grad_norm': 51.43569975832591, 'learning_rate': 1.793267739877542e-07, 'epoch': 0.7}
{'loss': 2.0909, 'grad_norm': 55.30860681194848, 'learning_rate': 1.7765439206848262e-07, 'epoch': 0.72}
{'loss': 2.0802, 'grad_norm': 54.021189813595456, 'learning_rate': 1.7592542271443886e-07, 'epoch': 0.75}
{'loss': 2.0334, 'grad_norm': 47.229963695694245, 'learning_rate': 1.7414112584071514e-07, 'epoch': 0.78}
{'loss': 2.0259, 'grad_norm': 55.47896675595009, 'learning_rate': 1.723028016800513e-07, 'epoch': 0.8}
{'loss': 2.0161, 'grad_norm': 55.46427526983953, 'learning_rate': 1.704117898353437e-07, 'epoch': 0.82}
{'loss': 1.982, 'grad_norm': 52.98068593860866, 'learning_rate': 1.6846946830346493e-07, 'epoch': 0.85}
{'loss': 1.9899, 'grad_norm': 54.4381149459159, 'learning_rate': 1.6647725247110551e-07, 'epoch': 0.88}
{'loss': 1.9409, 'grad_norm': 53.910568754457934, 'learning_rate': 1.6443659408336915e-07, 'epoch': 0.9}
{'loss': 1.9462, 'grad_norm': 55.793352852585016, 'learning_rate': 1.6234898018587336e-07, 'epoch': 0.93}
{'loss': 1.9254, 'grad_norm': 50.91206829953658, 'learning_rate': 1.602159320411264e-07, 'epoch': 0.95}
{'loss': 1.9143, 'grad_norm': 55.21324063329015, 'learning_rate': 1.5803900401996985e-07, 'epoch': 0.97}
{'loss': 1.9168, 'grad_norm': 53.933882743408184, 'learning_rate': 1.5581978246889524e-07, 'epoch': 1.0}
{'loss': 1.8884, 'grad_norm': 42.59564973799368, 'learning_rate': 1.535598845540591e-07, 'epoch': 1.02}
{'loss': 1.9013, 'grad_norm': 48.50787796026733, 'learning_rate': 1.5126095708284022e-07, 'epoch': 1.05}
{'loss': 1.8752, 'grad_norm': 50.834656201358065, 'learning_rate': 1.4892467530379636e-07, 'epoch': 1.07}
{'loss': 1.8813, 'grad_norm': 54.4046087715173, 'learning_rate': 1.4655274168589633e-07, 'epoch': 1.1}
{'loss': 1.833, 'grad_norm': 52.1198766352688, 'learning_rate': 1.4414688467791557e-07, 'epoch': 1.12}
{'loss': 1.8368, 'grad_norm': 49.03488438806774, 'learning_rate': 1.4170885744890083e-07, 'epoch': 1.15}
{'loss': 1.8405, 'grad_norm': 53.964915696806536, 'learning_rate': 1.3924043661062014e-07, 'epoch': 1.18}
{'loss': 1.8281, 'grad_norm': 50.67240759296299, 'learning_rate': 1.3674342092293063e-07, 'epoch': 1.2}
{'loss': 1.8188, 'grad_norm': 54.105091127689164, 'learning_rate': 1.342196299830062e-07, 'epoch': 1.23}
{'loss': 1.7976, 'grad_norm': 50.2112892671521, 'learning_rate': 1.316709028993813e-07, 'epoch': 1.25}
{'loss': 1.7874, 'grad_norm': 52.382739068606426, 'learning_rate': 1.2909909695177645e-07, 'epoch': 1.27}
{'loss': 1.7821, 'grad_norm': 53.75638889556691, 'learning_rate': 1.2650608623768219e-07, 'epoch': 1.3}
{'loss': 1.7753, 'grad_norm': 51.875943490739466, 'learning_rate': 1.2389376030668793e-07, 'epoch': 1.32}
{'loss': 1.7676, 'grad_norm': 50.43093021061195, 'learning_rate': 1.2126402278355063e-07, 'epoch': 1.35}
{'loss': 1.7556, 'grad_norm': 50.43798083801146, 'learning_rate': 1.186187899810066e-07, 'epoch': 1.38}
{'loss': 1.7516, 'grad_norm': 47.78084588949183, 'learning_rate': 1.1595998950333792e-07, 'epoch': 1.4}
{'loss': 1.7657, 'grad_norm': 53.035274613617744, 'learning_rate': 1.1328955884171018e-07, 'epoch': 1.43}
{'loss': 1.7361, 'grad_norm': 50.95843312488666, 'learning_rate': 1.1060944396230581e-07, 'epoch': 1.45}
{'loss': 1.7224, 'grad_norm': 52.633819821198216, 'learning_rate': 1.0792159788828132e-07, 'epoch': 1.48}
{'loss': 1.7019, 'grad_norm': 51.31545218877137, 'learning_rate': 1.0522797927658249e-07, 'epoch': 1.5}
{'loss': 1.7038, 'grad_norm': 49.96225214406579, 'learning_rate': 1.0253055099065371e-07, 'epoch': 1.52}
{'loss': 1.7059, 'grad_norm': 49.87745397352326, 'learning_rate': 9.983127867008237e-08, 'epoch': 1.55}
{'loss': 1.6953, 'grad_norm': 50.3795357058619, 'learning_rate': 9.713212929822003e-08, 'epoch': 1.57}
{'loss': 1.6968, 'grad_norm': 49.7624381885159, 'learning_rate': 9.443506976882442e-08, 'epoch': 1.6}
{'loss': 1.6944, 'grad_norm': 52.40647807488265, 'learning_rate': 9.174206545276677e-08, 'epoch': 1.62}
{'loss': 1.6762, 'grad_norm': 50.711200381366204, 'learning_rate': 8.905507876584892e-08, 'epoch': 1.65}
{'loss': 1.6953, 'grad_norm': 49.34454846473086, 'learning_rate': 8.637606773877369e-08, 'epoch': 1.68}
{'loss': 1.6848, 'grad_norm': 50.91157667900486, 'learning_rate': 8.37069845903108e-08, 'epoch': 1.7}
{'loss': 1.683, 'grad_norm': 50.12591824809294, 'learning_rate': 8.104977430469803e-08, 'epoch': 1.73}
{'loss': 1.6715, 'grad_norm': 49.39954415691844, 'learning_rate': 7.840637321431413e-08, 'epoch': 1.75}
{'loss': 1.6681, 'grad_norm': 48.64974233363224, 'learning_rate': 7.577870758865645e-08, 'epoch': 1.77}
{'loss': 1.6635, 'grad_norm': 48.296441169262316, 'learning_rate': 7.316869223065155e-08, 'epoch': 1.8}
{'loss': 1.6458, 'grad_norm': 48.78429729106206, 'learning_rate': 7.05782290813216e-08, 'epoch': 1.82}
{'loss': 1.6544, 'grad_norm': 48.2864790207065, 'learning_rate': 6.800920583382328e-08, 'epoch': 1.85}
{'loss': 1.632, 'grad_norm': 48.03263368239633, 'learning_rate': 6.546349455786925e-08, 'epoch': 1.88}
{'loss': 1.6237, 'grad_norm': 50.031503230727274, 'learning_rate': 6.294295033553454e-08, 'epoch': 1.9}
{'loss': 1.6206, 'grad_norm': 43.41272035348085, 'learning_rate': 6.044940990944214e-08, 'epoch': 1.93}
{'loss': 1.6399, 'grad_norm': 50.35577946674061, 'learning_rate': 5.7984690344312494e-08, 'epoch': 1.95}
{'loss': 1.6255, 'grad_norm': 49.43594588636894, 'learning_rate': 5.555058770285246e-08, 'epoch': 1.98}
{'loss': 1.6353, 'grad_norm': 49.785570823897466, 'learning_rate': 5.3148875736948904e-08, 'epoch': 2.0}
{'loss': 1.6094, 'grad_norm': 47.67238171417079, 'learning_rate': 5.078130459512004e-08, 'epoch': 2.02}
{'loss': 1.6232, 'grad_norm': 43.92002598760701, 'learning_rate': 4.844959954716699e-08, 'epoch': 2.05}
{'loss': 1.6204, 'grad_norm': 51.59623409479357, 'learning_rate': 4.615545972695447e-08, 'epoch': 2.08}
{'loss': 1.6268, 'grad_norm': 46.71244234981321, 'learning_rate': 4.3900556894237115e-08, 'epoch': 2.1}
{'loss': 1.6229, 'grad_norm': 50.1115503027664, 'learning_rate': 4.168653421643368e-08, 'epoch': 2.12}
{'loss': 1.607, 'grad_norm': 51.48378236732892, 'learning_rate': 3.9515005071236277e-08, 'epoch': 2.15}
{'loss': 1.6078, 'grad_norm': 48.027781610360044, 'learning_rate': 3.7387551870928226e-08, 'epoch': 2.17}
{'loss': 1.6081, 'grad_norm': 47.61209876446553, 'learning_rate': 3.530572490926621e-08, 'epoch': 2.2}
{'loss': 1.6006, 'grad_norm': 48.568485779581906, 'learning_rate': 3.3271041231767706e-08, 'epoch': 2.23}
{'loss': 1.5984, 'grad_norm': 47.34031642857197, 'learning_rate': 3.128498353022666e-08, 'epoch': 2.25}
{'loss': 1.5836, 'grad_norm': 48.335258632718, 'learning_rate': 2.9348999062262947e-08, 'epoch': 2.27}
{'loss': 1.6003, 'grad_norm': 46.57366451880666, 'learning_rate': 2.7464498596693174e-08, 'epoch': 2.3}
{'loss': 1.6015, 'grad_norm': 50.75845990497161, 'learning_rate': 2.5632855385491036e-08, 'epoch': 2.33}
{'loss': 1.6007, 'grad_norm': 49.8160537676826, 'learning_rate': 2.3855404163086556e-08, 'epoch': 2.35}
{'loss': 1.5902, 'grad_norm': 47.5957269706841, 'learning_rate': 2.2133440173733576e-08, 'epoch': 2.38}
{'loss': 1.6098, 'grad_norm': 50.36898466350128, 'learning_rate': 2.0468218227653745e-08, 'epoch': 2.4}
{'loss': 1.6143, 'grad_norm': 48.47966895314724, 'learning_rate': 1.8860951786645517e-08, 'epoch': 2.42}
{'loss': 1.5653, 'grad_norm': 49.64402436063725, 'learning_rate': 1.7312812079823803e-08, 'epoch': 2.45}
{'loss': 1.5721, 'grad_norm': 48.74251811460458, 'learning_rate': 1.5824927250135133e-08, 'epoch': 2.48}
{'loss': 1.5937, 'grad_norm': 51.207550768665286, 'learning_rate': 1.4398381532269998e-08, 'epoch': 2.5}
{'loss': 1.5848, 'grad_norm': 47.0401842371286, 'learning_rate': 1.3034214462571492e-08, 'epoch': 2.52}
{'loss': 1.579, 'grad_norm': 49.61525051227933, 'learning_rate': 1.1733420121516192e-08, 'epoch': 2.55}
{'loss': 1.5977, 'grad_norm': 48.12153744467633, 'learning_rate': 1.0496946409318975e-08, 'epoch': 2.58}
{'loss': 1.58, 'grad_norm': 46.91153457019708, 'learning_rate': 9.325694355189817e-09, 'epoch': 2.6}
{'loss': 1.593, 'grad_norm': 47.23391969912535, 'learning_rate': 8.220517460745912e-09, 'epoch': 2.62}
{'loss': 1.5799, 'grad_norm': 51.434282379252906, 'learning_rate': 7.182221078057648e-09, 'epoch': 2.65}
{'loss': 1.5899, 'grad_norm': 50.02229606060768, 'learning_rate': 6.2115618227814745e-09, 'epoch': 2.67}
{'loss': 1.5948, 'grad_norm': 48.0816248318692, 'learning_rate': 5.309247022807395e-09, 'epoch': 2.7}
{'loss': 1.5734, 'grad_norm': 45.51568657379518, 'learning_rate': 4.475934202823062e-09, 'epoch': 2.73}
{'loss': 1.5933, 'grad_norm': 50.3099779663203, 'learning_rate': 3.712230605169675e-09, 'epoch': 2.75}
{'loss': 1.5843, 'grad_norm': 45.64709534569497, 'learning_rate': 3.0186927473392467e-09, 'epoch': 2.77}
{'loss': 1.5868, 'grad_norm': 48.86100669897197, 'learning_rate': 2.3958260164354203e-09, 'epoch': 2.8}
{'loss': 1.5641, 'grad_norm': 48.4376071035955, 'learning_rate': 1.8440843008934558e-09, 'epoch': 2.83}
{'loss': 1.5736, 'grad_norm': 47.56227318067195, 'learning_rate': 1.3638696597277677e-09, 'epoch': 2.85}
{'loss': 1.5874, 'grad_norm': 49.30603674905093, 'learning_rate': 9.555320295479564e-10, 'epoch': 2.88}
{'loss': 1.5906, 'grad_norm': 48.79307004554535, 'learning_rate': 6.193689695570436e-10, 'epoch': 2.9}
{'loss': 1.5812, 'grad_norm': 48.550925725662474, 'learning_rate': 3.556254447173668e-10, 'epoch': 2.92}
{'loss': 1.5923, 'grad_norm': 48.11475497285518, 'learning_rate': 1.6449364724255842e-10, 'epoch': 2.95}
{'loss': 1.592, 'grad_norm': 49.7539202648582, 'learning_rate': 4.6112856545332813e-11, 'epoch': 2.98}
{'loss': 1.5766, 'grad_norm': 47.10721009710457, 'learning_rate': 5.693377433835244e-13, 'epoch': 3.0}
{'train_runtime': 9721.051, 'train_samples_per_second': 1.579, 'train_steps_per_second': 0.099, 'train_loss': 2.0606057902177173, 'epoch': 3.0}
