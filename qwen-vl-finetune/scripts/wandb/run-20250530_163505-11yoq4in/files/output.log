 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                    | 200/960 [28:44<1:49:01,  8.61s/it][34m[1mwandb[0m: [33mWARNING[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.
[2025-05-30 16:35:20,211] [WARNING] [stage3.py:2148:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 3.7444, 'grad_norm': 300.311898520488, 'learning_rate': 4.827586206896552e-08, 'epoch': 0.03}
{'loss': 3.566, 'grad_norm': 231.0398263726493, 'learning_rate': 1.0344827586206897e-07, 'epoch': 0.05}
{'loss': 3.5497, 'grad_norm': 211.83444533749085, 'learning_rate': 1.5862068965517243e-07, 'epoch': 0.07}
{'loss': 3.2365, 'grad_norm': 128.57458467579139, 'learning_rate': 1.9999772265550936e-07, 'epoch': 0.1}
{'loss': 3.1411, 'grad_norm': 64.42061077538693, 'learning_rate': 1.99943071573867e-07, 'epoch': 0.12}
{'loss': 2.9651, 'grad_norm': 64.97800718821483, 'learning_rate': 1.9981559110172944e-07, 'epoch': 0.15}
{'loss': 2.8839, 'grad_norm': 61.83555159336481, 'learning_rate': 1.99615374135232e-07, 'epoch': 0.17}
{'loss': 2.8093, 'grad_norm': 61.391071591754645, 'learning_rate': 1.9934256657422944e-07, 'epoch': 0.2}
{'loss': 2.7044, 'grad_norm': 53.29941778160717, 'learning_rate': 1.9899736721597784e-07, 'epoch': 0.23}
{'loss': 2.6169, 'grad_norm': 42.358248988542336, 'learning_rate': 1.9858002761026912e-07, 'epoch': 0.25}
{'loss': 2.5435, 'grad_norm': 36.925268101932836, 'learning_rate': 1.9809085187612465e-07, 'epoch': 0.28}
{'loss': 2.471, 'grad_norm': 37.88367796127207, 'learning_rate': 1.9753019648018085e-07, 'epoch': 0.3}
{'loss': 2.3749, 'grad_norm': 34.657187322726756, 'learning_rate': 1.96898469976929e-07, 'epoch': 0.33}
{'loss': 2.339, 'grad_norm': 33.08395168959039, 'learning_rate': 1.96196132710998e-07, 'epoch': 0.35}
{'loss': 2.2716, 'grad_norm': 36.7100462709202, 'learning_rate': 1.9542369648169715e-07, 'epoch': 0.38}
{'loss': 2.2261, 'grad_norm': 32.243851684718365, 'learning_rate': 1.9458172417006346e-07, 'epoch': 0.4}
{'loss': 2.2065, 'grad_norm': 33.918643304148766, 'learning_rate': 1.9367082932868553e-07, 'epoch': 0.42}
{'loss': 2.1166, 'grad_norm': 28.37763650786182, 'learning_rate': 1.9269167573460216e-07, 'epoch': 0.45}
{'loss': 2.076, 'grad_norm': 25.79653023891748, 'learning_rate': 1.9164497690560243e-07, 'epoch': 0.47}
{'loss': 2.0219, 'grad_norm': 26.739339457726025, 'learning_rate': 1.9053149558027887e-07, 'epoch': 0.5}
{'loss': 2.0317, 'grad_norm': 28.707979987601192, 'learning_rate': 1.8935204316221328e-07, 'epoch': 0.53}
{'loss': 1.9583, 'grad_norm': 30.361167766072775, 'learning_rate': 1.8810747912869977e-07, 'epoch': 0.55}
{'loss': 1.948, 'grad_norm': 29.56357820293893, 'learning_rate': 1.867987104044363e-07, 'epoch': 0.57}
{'loss': 1.9124, 'grad_norm': 26.662122981698793, 'learning_rate': 1.854266907006405e-07, 'epoch': 0.6}
{'loss': 1.9098, 'grad_norm': 25.426293015678713, 'learning_rate': 1.8399241982007208e-07, 'epoch': 0.62}
                                                                                                                                
{'loss': 1.8857, 'grad_norm': 30.582074256529527, 'learning_rate': 1.8249694292846783e-07, 'epoch': 0.65}
{'loss': 1.8498, 'grad_norm': 27.541870822389697, 'learning_rate': 1.8094134979292012e-07, 'epoch': 0.68}
{'loss': 1.8305, 'grad_norm': 29.644628439520165, 'learning_rate': 1.793267739877542e-07, 'epoch': 0.7}
{'loss': 1.8081, 'grad_norm': 30.52498816039253, 'learning_rate': 1.7765439206848262e-07, 'epoch': 0.72}
{'loss': 1.7896, 'grad_norm': 30.99520042799426, 'learning_rate': 1.7592542271443886e-07, 'epoch': 0.75}
{'loss': 1.8052, 'grad_norm': 31.862748587667646, 'learning_rate': 1.7414112584071514e-07, 'epoch': 0.78}
{'loss': 1.7688, 'grad_norm': 31.99209389713171, 'learning_rate': 1.723028016800513e-07, 'epoch': 0.8}
{'loss': 1.7415, 'grad_norm': 29.96099250514665, 'learning_rate': 1.704117898353437e-07, 'epoch': 0.82}
{'loss': 1.7364, 'grad_norm': 31.00077518472362, 'learning_rate': 1.6846946830346493e-07, 'epoch': 0.85}
{'loss': 1.7351, 'grad_norm': 29.827502573715602, 'learning_rate': 1.6647725247110551e-07, 'epoch': 0.88}
{'loss': 1.7115, 'grad_norm': 31.01104003862402, 'learning_rate': 1.6443659408336915e-07, 'epoch': 0.9}
{'loss': 1.7105, 'grad_norm': 32.875655670981594, 'learning_rate': 1.6234898018587336e-07, 'epoch': 0.93}
{'loss': 1.6889, 'grad_norm': 32.76491571599318, 'learning_rate': 1.602159320411264e-07, 'epoch': 0.95}
{'loss': 1.682, 'grad_norm': 30.794123058439276, 'learning_rate': 1.5803900401996985e-07, 'epoch': 0.97}
{'loss': 1.6804, 'grad_norm': 29.393635535936735, 'learning_rate': 1.5581978246889524e-07, 'epoch': 1.0}
{'loss': 1.6893, 'grad_norm': 29.775886889200226, 'learning_rate': 1.535598845540591e-07, 'epoch': 1.02}
{'loss': 1.6525, 'grad_norm': 29.33180001170804, 'learning_rate': 1.5126095708284022e-07, 'epoch': 1.05}
{'loss': 1.6455, 'grad_norm': 32.797401493244315, 'learning_rate': 1.4892467530379636e-07, 'epoch': 1.07}
{'loss': 1.6293, 'grad_norm': 29.02277173653697, 'learning_rate': 1.4655274168589633e-07, 'epoch': 1.1}
{'loss': 1.64, 'grad_norm': 29.336460676066256, 'learning_rate': 1.4414688467791557e-07, 'epoch': 1.12}
{'loss': 1.6154, 'grad_norm': 32.30403008247486, 'learning_rate': 1.4170885744890083e-07, 'epoch': 1.15}
{'loss': 1.6144, 'grad_norm': 31.923881960940154, 'learning_rate': 1.3924043661062014e-07, 'epoch': 1.18}
{'loss': 1.6132, 'grad_norm': 30.973949308773904, 'learning_rate': 1.3674342092293063e-07, 'epoch': 1.2}
{'loss': 1.5871, 'grad_norm': 29.01501069129445, 'learning_rate': 1.342196299830062e-07, 'epoch': 1.23}
{'loss': 1.6059, 'grad_norm': 31.99926429348789, 'learning_rate': 1.316709028993813e-07, 'epoch': 1.25}
{'loss': 1.6004, 'grad_norm': 28.56988921918333, 'learning_rate': 1.2909909695177645e-07, 'epoch': 1.27}
{'loss': 1.5808, 'grad_norm': 31.836812599858376, 'learning_rate': 1.2650608623768219e-07, 'epoch': 1.3}
{'loss': 1.5723, 'grad_norm': 34.25938271483214, 'learning_rate': 1.2389376030668793e-07, 'epoch': 1.32}
{'loss': 1.5752, 'grad_norm': 29.654909972621862, 'learning_rate': 1.2126402278355063e-07, 'epoch': 1.35}
{'loss': 1.5443, 'grad_norm': 27.10222378616253, 'learning_rate': 1.186187899810066e-07, 'epoch': 1.38}
{'loss': 1.5549, 'grad_norm': 32.45414046234761, 'learning_rate': 1.1595998950333792e-07, 'epoch': 1.4}
{'loss': 1.551, 'grad_norm': 30.260056760421868, 'learning_rate': 1.1328955884171018e-07, 'epoch': 1.43}
{'loss': 1.561, 'grad_norm': 28.507639817312974, 'learning_rate': 1.1060944396230581e-07, 'epoch': 1.45}
{'loss': 1.5614, 'grad_norm': 24.77788758320762, 'learning_rate': 1.0792159788828132e-07, 'epoch': 1.48}
{'loss': 1.5398, 'grad_norm': 33.06404213265302, 'learning_rate': 1.0522797927658249e-07, 'epoch': 1.5}
{'loss': 1.5368, 'grad_norm': 30.70305331251723, 'learning_rate': 1.0253055099065371e-07, 'epoch': 1.52}
{'loss': 1.5189, 'grad_norm': 33.9402893949684, 'learning_rate': 9.983127867008237e-08, 'epoch': 1.55}
{'loss': 1.52, 'grad_norm': 34.65433903584722, 'learning_rate': 9.713212929822003e-08, 'epoch': 1.57}
{'loss': 1.5203, 'grad_norm': 32.38555576537085, 'learning_rate': 9.443506976882442e-08, 'epoch': 1.6}
{'loss': 1.5094, 'grad_norm': 31.35285000480619, 'learning_rate': 9.174206545276677e-08, 'epoch': 1.62}
{'loss': 1.493, 'grad_norm': 34.34518873204024, 'learning_rate': 8.905507876584892e-08, 'epoch': 1.65}
{'loss': 1.5102, 'grad_norm': 29.361856217896648, 'learning_rate': 8.637606773877369e-08, 'epoch': 1.68}
{'loss': 1.4861, 'grad_norm': 33.68580151444404, 'learning_rate': 8.37069845903108e-08, 'epoch': 1.7}
{'loss': 1.4908, 'grad_norm': 26.732200454280193, 'learning_rate': 8.104977430469803e-08, 'epoch': 1.73}
{'loss': 1.4939, 'grad_norm': 29.34288035981159, 'learning_rate': 7.840637321431413e-08, 'epoch': 1.75}
{'loss': 1.4831, 'grad_norm': 32.539579189830874, 'learning_rate': 7.577870758865645e-08, 'epoch': 1.77}
{'loss': 1.4948, 'grad_norm': 27.50815535155592, 'learning_rate': 7.316869223065155e-08, 'epoch': 1.8}
{'loss': 1.4757, 'grad_norm': 32.860001062256664, 'learning_rate': 7.05782290813216e-08, 'epoch': 1.82}
{'loss': 1.4561, 'grad_norm': 32.7108977183749, 'learning_rate': 6.800920583382328e-08, 'epoch': 1.85}
{'loss': 1.4932, 'grad_norm': 30.788362090042245, 'learning_rate': 6.546349455786925e-08, 'epoch': 1.88}
{'loss': 1.4588, 'grad_norm': 30.48227674234369, 'learning_rate': 6.294295033553454e-08, 'epoch': 1.9}
{'loss': 1.4586, 'grad_norm': 31.455077919263392, 'learning_rate': 6.044940990944214e-08, 'epoch': 1.93}
{'loss': 1.4724, 'grad_norm': 25.627271841106225, 'learning_rate': 5.7984690344312494e-08, 'epoch': 1.95}
{'loss': 1.4627, 'grad_norm': 31.655620407581726, 'learning_rate': 5.555058770285246e-08, 'epoch': 1.98}
{'loss': 1.4676, 'grad_norm': 33.02082560317173, 'learning_rate': 5.3148875736948904e-08, 'epoch': 2.0}
{'loss': 1.4491, 'grad_norm': 31.95636378756736, 'learning_rate': 5.078130459512004e-08, 'epoch': 2.02}
{'loss': 1.4501, 'grad_norm': 32.92499395657435, 'learning_rate': 4.844959954716699e-08, 'epoch': 2.05}
{'loss': 1.4445, 'grad_norm': 29.908284906622672, 'learning_rate': 4.615545972695447e-08, 'epoch': 2.08}
{'loss': 1.4531, 'grad_norm': 29.050738738213482, 'learning_rate': 4.3900556894237115e-08, 'epoch': 2.1}
{'loss': 1.4594, 'grad_norm': 29.52899674423015, 'learning_rate': 4.168653421643368e-08, 'epoch': 2.12}
{'loss': 1.4528, 'grad_norm': 27.85497236850835, 'learning_rate': 3.9515005071236277e-08, 'epoch': 2.15}
{'loss': 1.4429, 'grad_norm': 29.924907291608303, 'learning_rate': 3.7387551870928226e-08, 'epoch': 2.17}
{'loss': 1.4481, 'grad_norm': 26.11784467419953, 'learning_rate': 3.530572490926621e-08, 'epoch': 2.2}
{'loss': 1.4335, 'grad_norm': 31.263802446491155, 'learning_rate': 3.3271041231767706e-08, 'epoch': 2.23}
{'loss': 1.4434, 'grad_norm': 25.98792483104749, 'learning_rate': 3.128498353022666e-08, 'epoch': 2.25}
{'loss': 1.448, 'grad_norm': 32.955705311063895, 'learning_rate': 2.9348999062262947e-08, 'epoch': 2.27}
{'loss': 1.4509, 'grad_norm': 27.41986346476764, 'learning_rate': 2.7464498596693174e-08, 'epoch': 2.3}
{'loss': 1.4362, 'grad_norm': 30.05299319009468, 'learning_rate': 2.5632855385491036e-08, 'epoch': 2.33}
{'loss': 1.4184, 'grad_norm': 30.47000586182006, 'learning_rate': 2.3855404163086556e-08, 'epoch': 2.35}
{'loss': 1.4444, 'grad_norm': 28.5977249613649, 'learning_rate': 2.2133440173733576e-08, 'epoch': 2.38}
{'loss': 1.4465, 'grad_norm': 32.0036966857011, 'learning_rate': 2.0468218227653745e-08, 'epoch': 2.4}
{'loss': 1.4358, 'grad_norm': 30.21981738058169, 'learning_rate': 1.8860951786645517e-08, 'epoch': 2.42}
{'loss': 1.4275, 'grad_norm': 26.12091138698755, 'learning_rate': 1.7312812079823803e-08, 'epoch': 2.45}
{'loss': 1.4352, 'grad_norm': 29.784803453144917, 'learning_rate': 1.5824927250135133e-08, 'epoch': 2.48}
{'loss': 1.4377, 'grad_norm': 28.44703310190434, 'learning_rate': 1.4398381532269998e-08, 'epoch': 2.5}
{'loss': 1.4363, 'grad_norm': 30.744395810493135, 'learning_rate': 1.3034214462571492e-08, 'epoch': 2.52}
{'loss': 1.4336, 'grad_norm': 31.341056448208967, 'learning_rate': 1.1733420121516192e-08, 'epoch': 2.55}
{'loss': 1.426, 'grad_norm': 31.915929715880953, 'learning_rate': 1.0496946409318975e-08, 'epoch': 2.58}
{'loss': 1.4292, 'grad_norm': 28.34701144988896, 'learning_rate': 9.325694355189817e-09, 'epoch': 2.6}
{'loss': 1.4385, 'grad_norm': 26.361863072430012, 'learning_rate': 8.220517460745912e-09, 'epoch': 2.62}
{'loss': 1.4229, 'grad_norm': 32.263156657159364, 'learning_rate': 7.182221078057648e-09, 'epoch': 2.65}
{'loss': 1.4224, 'grad_norm': 30.514577401264233, 'learning_rate': 6.2115618227814745e-09, 'epoch': 2.67}
{'loss': 1.4182, 'grad_norm': 27.677150246877595, 'learning_rate': 5.309247022807395e-09, 'epoch': 2.7}
{'loss': 1.4173, 'grad_norm': 25.572700029255508, 'learning_rate': 4.475934202823062e-09, 'epoch': 2.73}
{'loss': 1.4192, 'grad_norm': 30.734837190657707, 'learning_rate': 3.712230605169675e-09, 'epoch': 2.75}
{'loss': 1.4262, 'grad_norm': 31.15245492667259, 'learning_rate': 3.0186927473392467e-09, 'epoch': 2.77}
{'loss': 1.4334, 'grad_norm': 28.889710523521234, 'learning_rate': 2.3958260164354203e-09, 'epoch': 2.8}
{'loss': 1.4342, 'grad_norm': 29.229712130479683, 'learning_rate': 1.8440843008934558e-09, 'epoch': 2.83}
{'loss': 1.436, 'grad_norm': 24.865999772227912, 'learning_rate': 1.3638696597277677e-09, 'epoch': 2.85}
{'loss': 1.4177, 'grad_norm': 30.97618306754462, 'learning_rate': 9.555320295479564e-10, 'epoch': 2.88}
{'loss': 1.4137, 'grad_norm': 27.86447178240502, 'learning_rate': 6.193689695570436e-10, 'epoch': 2.9}
{'loss': 1.4273, 'grad_norm': 26.458234904698884, 'learning_rate': 3.556254447173668e-10, 'epoch': 2.92}
{'loss': 1.4421, 'grad_norm': 32.655466485640396, 'learning_rate': 1.6449364724255842e-10, 'epoch': 2.95}
{'loss': 1.41, 'grad_norm': 29.191759564422703, 'learning_rate': 4.6112856545332813e-11, 'epoch': 2.98}
{'loss': 1.429, 'grad_norm': 30.54291656632262, 'learning_rate': 5.693377433835244e-13, 'epoch': 3.0}
{'train_runtime': 9782.6233, 'train_samples_per_second': 1.57, 'train_steps_per_second': 0.098, 'train_loss': 1.7431280096371968, 'epoch': 3.0}
