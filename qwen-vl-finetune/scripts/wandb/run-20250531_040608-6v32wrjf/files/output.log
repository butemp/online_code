                                                                                                                                
[2025-05-31 04:06:24,580] [WARNING] [stage3.py:2148:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 3.6508, 'grad_norm': 249.46914416565082, 'learning_rate': 4.827586206896552e-07, 'epoch': 0.03}
{'loss': 3.1205, 'grad_norm': 52.33088634288723, 'learning_rate': 1.0344827586206896e-06, 'epoch': 0.05}
{'loss': 2.7152, 'grad_norm': 47.234164138022855, 'learning_rate': 1.5862068965517242e-06, 'epoch': 0.07}
{'loss': 2.4062, 'grad_norm': 34.052969676976474, 'learning_rate': 1.9999772265550934e-06, 'epoch': 0.1}
{'loss': 2.0419, 'grad_norm': 29.951509813352853, 'learning_rate': 1.99943071573867e-06, 'epoch': 0.12}
{'loss': 1.8676, 'grad_norm': 32.345736729673064, 'learning_rate': 1.9981559110172943e-06, 'epoch': 0.15}
{'loss': 1.7408, 'grad_norm': 28.988556448870064, 'learning_rate': 1.99615374135232e-06, 'epoch': 0.17}
{'loss': 1.6473, 'grad_norm': 36.37570520666443, 'learning_rate': 1.9934256657422944e-06, 'epoch': 0.2}
{'loss': 1.5547, 'grad_norm': 32.97694001536856, 'learning_rate': 1.9899736721597786e-06, 'epoch': 0.23}
{'loss': 1.464, 'grad_norm': 30.954818874635716, 'learning_rate': 1.985800276102691e-06, 'epoch': 0.25}
{'loss': 1.4049, 'grad_norm': 28.67697732855827, 'learning_rate': 1.9809085187612464e-06, 'epoch': 0.28}
{'loss': 1.3396, 'grad_norm': 31.021607829583562, 'learning_rate': 1.9753019648018085e-06, 'epoch': 0.3}
{'loss': 1.2605, 'grad_norm': 27.222038960145593, 'learning_rate': 1.96898469976929e-06, 'epoch': 0.33}
{'loss': 1.2019, 'grad_norm': 27.81546044621095, 'learning_rate': 1.96196132710998e-06, 'epoch': 0.35}
{'loss': 1.1506, 'grad_norm': 23.73013199907541, 'learning_rate': 1.9542369648169714e-06, 'epoch': 0.38}
{'loss': 1.0843, 'grad_norm': 25.260079511814823, 'learning_rate': 1.9458172417006346e-06, 'epoch': 0.4}
{'loss': 1.046, 'grad_norm': 26.895174948111684, 'learning_rate': 1.9367082932868553e-06, 'epoch': 0.42}
{'loss': 0.9857, 'grad_norm': 22.486489073251732, 'learning_rate': 1.9269167573460217e-06, 'epoch': 0.45}
{'loss': 0.9329, 'grad_norm': 23.632521127400057, 'learning_rate': 1.916449769056024e-06, 'epoch': 0.47}
{'loss': 0.8865, 'grad_norm': 16.8545448139976, 'learning_rate': 1.9053149558027885e-06, 'epoch': 0.5}
{'loss': 0.8783, 'grad_norm': 15.511658065800082, 'learning_rate': 1.8935204316221326e-06, 'epoch': 0.53}
{'loss': 0.827, 'grad_norm': 12.589041308378809, 'learning_rate': 1.8810747912869979e-06, 'epoch': 0.55}
{'loss': 0.8541, 'grad_norm': 7.355820988062574, 'learning_rate': 1.867987104044363e-06, 'epoch': 0.57}
{'loss': 0.8099, 'grad_norm': 8.385289795877696, 'learning_rate': 1.854266907006405e-06, 'epoch': 0.6}
{'loss': 0.8044, 'grad_norm': 8.417593587818299, 'learning_rate': 1.8399241982007208e-06, 'epoch': 0.62}
{'loss': 0.8142, 'grad_norm': 7.373192035207956, 'learning_rate': 1.8249694292846783e-06, 'epoch': 0.65}
{'loss': 0.7611, 'grad_norm': 6.705451928563313, 'learning_rate': 1.809413497929201e-06, 'epoch': 0.68}
{'loss': 0.7687, 'grad_norm': 5.757233819205469, 'learning_rate': 1.793267739877542e-06, 'epoch': 0.7}
{'loss': 0.7642, 'grad_norm': 5.097569545269477, 'learning_rate': 1.7765439206848263e-06, 'epoch': 0.72}
{'loss': 0.7563, 'grad_norm': 5.688039454410617, 'learning_rate': 1.7592542271443887e-06, 'epoch': 0.75}
{'loss': 0.7625, 'grad_norm': 4.6017758510216495, 'learning_rate': 1.7414112584071515e-06, 'epoch': 0.78}
{'loss': 0.7454, 'grad_norm': 5.041527348287831, 'learning_rate': 1.723028016800513e-06, 'epoch': 0.8}
{'loss': 0.7402, 'grad_norm': 4.654406883906985, 'learning_rate': 1.704117898353437e-06, 'epoch': 0.82}
{'loss': 0.7508, 'grad_norm': 4.576458546116239, 'learning_rate': 1.6846946830346494e-06, 'epoch': 0.85}
{'loss': 0.7359, 'grad_norm': 4.152763292797687, 'learning_rate': 1.6647725247110551e-06, 'epoch': 0.88}
{'loss': 0.7577, 'grad_norm': 3.4696391713267527, 'learning_rate': 1.6443659408336913e-06, 'epoch': 0.9}
{'loss': 0.7505, 'grad_norm': 3.472241411538819, 'learning_rate': 1.6234898018587336e-06, 'epoch': 0.93}
{'loss': 0.7687, 'grad_norm': 4.266795182569513, 'learning_rate': 1.6021593204112638e-06, 'epoch': 0.95}
{'loss': 0.7545, 'grad_norm': 3.6474724766305644, 'learning_rate': 1.5803900401996986e-06, 'epoch': 0.97}
{'loss': 0.7436, 'grad_norm': 3.773154297306994, 'learning_rate': 1.5581978246889524e-06, 'epoch': 1.0}
{'loss': 0.7483, 'grad_norm': 3.5120599836038724, 'learning_rate': 1.5355988455405911e-06, 'epoch': 1.02}
{'loss': 0.7627, 'grad_norm': 3.844663380133691, 'learning_rate': 1.5126095708284022e-06, 'epoch': 1.05}
{'loss': 0.7048, 'grad_norm': 3.467499754898116, 'learning_rate': 1.4892467530379638e-06, 'epoch': 1.07}
{'loss': 0.745, 'grad_norm': 3.439197696896509, 'learning_rate': 1.4655274168589633e-06, 'epoch': 1.1}
{'loss': 0.7793, 'grad_norm': 3.6684501240262493, 'learning_rate': 1.441468846779156e-06, 'epoch': 1.12}
{'loss': 0.7318, 'grad_norm': 3.697350877663165, 'learning_rate': 1.4170885744890082e-06, 'epoch': 1.15}
{'loss': 0.7257, 'grad_norm': 3.576106877511612, 'learning_rate': 1.3924043661062016e-06, 'epoch': 1.18}
{'loss': 0.7344, 'grad_norm': 3.2580086579799286, 'learning_rate': 1.3674342092293064e-06, 'epoch': 1.2}
{'loss': 0.7032, 'grad_norm': 3.890562337216881, 'learning_rate': 1.342196299830062e-06, 'epoch': 1.23}
{'loss': 0.7294, 'grad_norm': 3.6623745623074244, 'learning_rate': 1.316709028993813e-06, 'epoch': 1.25}
{'loss': 0.7269, 'grad_norm': 3.7700020741771656, 'learning_rate': 1.2909909695177645e-06, 'epoch': 1.27}
{'loss': 0.7123, 'grad_norm': 3.1848357881482334, 'learning_rate': 1.265060862376822e-06, 'epoch': 1.3}
{'loss': 0.7299, 'grad_norm': 3.11250215223518, 'learning_rate': 1.2389376030668794e-06, 'epoch': 1.32}
{'loss': 0.6777, 'grad_norm': 3.371474702563694, 'learning_rate': 1.2126402278355063e-06, 'epoch': 1.35}
{'loss': 0.7433, 'grad_norm': 3.807313291008721, 'learning_rate': 1.1861878998100661e-06, 'epoch': 1.38}
{'loss': 0.728, 'grad_norm': 3.4645314874827577, 'learning_rate': 1.1595998950333793e-06, 'epoch': 1.4}
{'loss': 0.7316, 'grad_norm': 3.0480735226436875, 'learning_rate': 1.132895588417102e-06, 'epoch': 1.43}
{'loss': 0.7023, 'grad_norm': 2.953747548684742, 'learning_rate': 1.106094439623058e-06, 'epoch': 1.45}
{'loss': 0.7384, 'grad_norm': 3.366680689042471, 'learning_rate': 1.0792159788828132e-06, 'epoch': 1.48}
{'loss': 0.7296, 'grad_norm': 3.187558742203535, 'learning_rate': 1.052279792765825e-06, 'epoch': 1.5}
{'loss': 0.6923, 'grad_norm': 3.3033587118503887, 'learning_rate': 1.0253055099065371e-06, 'epoch': 1.52}
{'loss': 0.7353, 'grad_norm': 3.184238427539781, 'learning_rate': 9.983127867008237e-07, 'epoch': 1.55}
{'loss': 0.6981, 'grad_norm': 3.419365977156128, 'learning_rate': 9.713212929822002e-07, 'epoch': 1.57}
{'loss': 0.7127, 'grad_norm': 3.043478717763007, 'learning_rate': 9.443506976882442e-07, 'epoch': 1.6}
{'loss': 0.7053, 'grad_norm': 3.2694675754873423, 'learning_rate': 9.174206545276677e-07, 'epoch': 1.62}
{'loss': 0.7094, 'grad_norm': 3.3410216545126636, 'learning_rate': 8.905507876584892e-07, 'epoch': 1.65}
{'loss': 0.6998, 'grad_norm': 3.610980361539025, 'learning_rate': 8.637606773877368e-07, 'epoch': 1.68}
{'loss': 0.7279, 'grad_norm': 3.0608561977798137, 'learning_rate': 8.37069845903108e-07, 'epoch': 1.7}
{'loss': 0.707, 'grad_norm': 3.229141761998156, 'learning_rate': 8.104977430469802e-07, 'epoch': 1.73}
{'loss': 0.691, 'grad_norm': 3.0918025752317058, 'learning_rate': 7.840637321431412e-07, 'epoch': 1.75}
{'loss': 0.7248, 'grad_norm': 2.8322445868832373, 'learning_rate': 7.577870758865645e-07, 'epoch': 1.77}
{'loss': 0.7101, 'grad_norm': 3.0768071973175997, 'learning_rate': 7.316869223065155e-07, 'epoch': 1.8}
{'loss': 0.7202, 'grad_norm': 3.0183554607469265, 'learning_rate': 7.05782290813216e-07, 'epoch': 1.82}
{'loss': 0.6853, 'grad_norm': 2.8372566791643012, 'learning_rate': 6.800920583382328e-07, 'epoch': 1.85}
{'loss': 0.714, 'grad_norm': 3.3906639163792174, 'learning_rate': 6.546349455786925e-07, 'epoch': 1.88}
{'loss': 0.6987, 'grad_norm': 3.2014118698974783, 'learning_rate': 6.294295033553454e-07, 'epoch': 1.9}
{'loss': 0.7049, 'grad_norm': 3.1492668893447764, 'learning_rate': 6.044940990944214e-07, 'epoch': 1.93}
{'loss': 0.6914, 'grad_norm': 3.3927796162757566, 'learning_rate': 5.798469034431249e-07, 'epoch': 1.95}
{'loss': 0.7186, 'grad_norm': 3.439331575360513, 'learning_rate': 5.555058770285246e-07, 'epoch': 1.98}
{'loss': 0.701, 'grad_norm': 3.2819938911908024, 'learning_rate': 5.314887573694891e-07, 'epoch': 2.0}
{'loss': 0.6968, 'grad_norm': 2.784536796233742, 'learning_rate': 5.078130459512005e-07, 'epoch': 2.02}
{'loss': 0.7077, 'grad_norm': 3.4754979914509074, 'learning_rate': 4.844959954716698e-07, 'epoch': 2.05}
{'loss': 0.7, 'grad_norm': 3.4878818739595716, 'learning_rate': 4.6155459726954473e-07, 'epoch': 2.08}
{'loss': 0.7054, 'grad_norm': 2.8260593892514088, 'learning_rate': 4.390055689423712e-07, 'epoch': 2.1}
{'loss': 0.7065, 'grad_norm': 3.4451497453090307, 'learning_rate': 4.1686534216433677e-07, 'epoch': 2.12}
{'loss': 0.6631, 'grad_norm': 2.916464019383449, 'learning_rate': 3.951500507123627e-07, 'epoch': 2.15}
{'loss': 0.6893, 'grad_norm': 2.787158043111881, 'learning_rate': 3.7387551870928226e-07, 'epoch': 2.17}
{'loss': 0.6807, 'grad_norm': 2.9794673910164806, 'learning_rate': 3.530572490926621e-07, 'epoch': 2.2}
{'loss': 0.6983, 'grad_norm': 3.2347770070412407, 'learning_rate': 3.3271041231767704e-07, 'epoch': 2.23}
{'loss': 0.6677, 'grad_norm': 2.953948012486731, 'learning_rate': 3.128498353022666e-07, 'epoch': 2.25}
{'loss': 0.7051, 'grad_norm': 2.9346462237497946, 'learning_rate': 2.934899906226295e-07, 'epoch': 2.27}
{'loss': 0.7076, 'grad_norm': 3.0689046770396047, 'learning_rate': 2.7464498596693174e-07, 'epoch': 2.3}
{'loss': 0.7121, 'grad_norm': 3.3860476142491103, 'learning_rate': 2.5632855385491037e-07, 'epoch': 2.33}
{'loss': 0.7024, 'grad_norm': 3.0140333411789695, 'learning_rate': 2.3855404163086556e-07, 'epoch': 2.35}
{'loss': 0.6917, 'grad_norm': 2.978539755950415, 'learning_rate': 2.2133440173733576e-07, 'epoch': 2.38}
{'loss': 0.6701, 'grad_norm': 2.8835961556579206, 'learning_rate': 2.0468218227653744e-07, 'epoch': 2.4}
{'loss': 0.6968, 'grad_norm': 2.8230362685176598, 'learning_rate': 1.886095178664552e-07, 'epoch': 2.42}
{'loss': 0.7026, 'grad_norm': 2.8427030624218026, 'learning_rate': 1.7312812079823802e-07, 'epoch': 2.45}
{'loss': 0.7033, 'grad_norm': 3.1258308680456337, 'learning_rate': 1.5824927250135135e-07, 'epoch': 2.48}
{'loss': 0.6651, 'grad_norm': 2.956940194978132, 'learning_rate': 1.4398381532269998e-07, 'epoch': 2.5}
{'loss': 0.6888, 'grad_norm': 3.483104987724956, 'learning_rate': 1.3034214462571492e-07, 'epoch': 2.52}
{'loss': 0.6791, 'grad_norm': 3.0088754315136943, 'learning_rate': 1.173342012151619e-07, 'epoch': 2.55}
{'loss': 0.6847, 'grad_norm': 2.863234775564637, 'learning_rate': 1.0496946409318974e-07, 'epoch': 2.58}
{'loss': 0.6795, 'grad_norm': 3.231461089569922, 'learning_rate': 9.325694355189816e-08, 'epoch': 2.6}
{'loss': 0.6982, 'grad_norm': 3.055874302127122, 'learning_rate': 8.220517460745912e-08, 'epoch': 2.62}
{'loss': 0.6887, 'grad_norm': 3.076565198954519, 'learning_rate': 7.182221078057649e-08, 'epoch': 2.65}
{'loss': 0.7087, 'grad_norm': 3.3597471629831213, 'learning_rate': 6.211561822781474e-08, 'epoch': 2.67}
{'loss': 0.6764, 'grad_norm': 3.119389782382855, 'learning_rate': 5.309247022807395e-08, 'epoch': 2.7}
{'loss': 0.6931, 'grad_norm': 2.8972815655681186, 'learning_rate': 4.4759342028230616e-08, 'epoch': 2.73}
{'loss': 0.6736, 'grad_norm': 3.3430105308554303, 'learning_rate': 3.712230605169675e-08, 'epoch': 2.75}
{'loss': 0.6822, 'grad_norm': 3.2966366892958945, 'learning_rate': 3.0186927473392464e-08, 'epoch': 2.77}
{'loss': 0.6844, 'grad_norm': 3.0817783308822935, 'learning_rate': 2.3958260164354205e-08, 'epoch': 2.8}
{'loss': 0.6979, 'grad_norm': 2.9843344668602256, 'learning_rate': 1.844084300893456e-08, 'epoch': 2.83}
{'loss': 0.7037, 'grad_norm': 2.8546033262550936, 'learning_rate': 1.3638696597277677e-08, 'epoch': 2.85}
{'loss': 0.7013, 'grad_norm': 2.9900023465144443, 'learning_rate': 9.555320295479564e-09, 'epoch': 2.88}
{'loss': 0.6978, 'grad_norm': 3.3003154044335443, 'learning_rate': 6.1936896955704365e-09, 'epoch': 2.9}
{'loss': 0.6822, 'grad_norm': 2.806651779305633, 'learning_rate': 3.5562544471736675e-09, 'epoch': 2.92}
100%|██████████████████████████████████████████████████████████████████████████████████████| 960/960 [2:44:38<00:00, 10.29s/it]
{'loss': 0.6994, 'grad_norm': 3.3125051332056517, 'learning_rate': 1.6449364724255842e-09, 'epoch': 2.95}
{'loss': 0.6664, 'grad_norm': 3.699362104536437, 'learning_rate': 4.611285654533281e-10, 'epoch': 2.98}
{'loss': 0.6782, 'grad_norm': 2.8777736818723723, 'learning_rate': 5.6933774338352444e-12, 'epoch': 3.0}
{'train_runtime': 10042.164, 'train_samples_per_second': 1.529, 'train_steps_per_second': 0.096, 'train_loss': 0.8784919892748196, 'epoch': 3.0}
